{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de115e5",
   "metadata": {},
   "source": [
    "#### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed8ce0",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a powerful machine learning technique that builds an ensemble of predictive models, typically decision trees, to optimize a loss function in a sequential manner. In Gradient Boosting, each new model incrementally corrects the errors made by previous models. The final prediction model is a weighted sum of these weaker models. This approach is used not only for regression but also for classification and other predictive tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa73328d",
   "metadata": {},
   "source": [
    "#### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8338815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.9847493141763362\n",
      "R2: 0.9980443215884244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.33)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr=GradientBoostingRegressor()\n",
    "gbr.fit(X_train,y_train)\n",
    "y_pred=gbr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "r2_scure = r2_score(y_test,y_pred)\n",
    "\n",
    "print(\"MSE:\",mse)\n",
    "print(\"R2:\",r2_scure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808408bf",
   "metadata": {},
   "source": [
    "#### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d862bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 200}\n",
      "Best Score: 19.883364714804376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the model and parameters\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", -best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea715a06",
   "metadata": {},
   "source": [
    "#### Q4. What is a weak learner in Gradient Boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cdeed9",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a \"weak learner\" is typically a simple model like a shallow decision tree. These trees are not very accurate on their own but are effective when combined into an ensemble. The weakness comes from the model's simplicity and limited depth, restricting its expressive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb742691",
   "metadata": {},
   "source": [
    "#### Q5. What is the intuition behind the Gradient Boosting algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2c0c0",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a \"weak learner\" is typically a simple model like a shallow decision tree. These trees are not very accurate on their own but are effective when combined into an ensemble. The weakness comes from the model's simplicity and limited depth, restricting its expressive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a623ef7",
   "metadata": {},
   "source": [
    "#### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96d336f",
   "metadata": {},
   "source": [
    "Gradient Boosting builds an ensemble of weak learners (typically decision trees) sequentially. Each tree in the sequence is trained to predict the residuals or errors of the previous ensemble. After a tree is trained, it contributes to the ensemble with a weighted prediction, with the weight often equivalent to the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6777c",
   "metadata": {},
   "source": [
    "#### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe7d4a",
   "metadata": {},
   "source": [
    "#### Steps:\n",
    "1.Create a Base Model: The first step is to create a base model, which provides an initial prediction for the target variable. In Gradient Boosting Regression, this base model typically starts with the average value of the target variable. This initial prediction acts as a starting point for the ensemble.\n",
    "\n",
    "2.Construct a Decision Tree: After creating the base model, a decision tree is constructed to capture the patterns in the data that the base model fails to capture. This decision tree is trained to predict the residuals or errors of the base model's predictions. The residuals are the differences between the actual target values and the predictions made by the base model.\n",
    "\n",
    "3.Compute Residuals: Once the decision tree is trained, it predicts the residuals for each data point. These residuals represent the errors made by the base model, and the decision tree aims to correct these errors in its predictions.\n",
    "\n",
    "4.Update Predictions: The predictions of the decision tree are then added to the predictions of the base model. However, the contribution of the decision tree's predictions to the ensemble is controlled by a hyperparameter called the learning rate. Typically, the learning rate is less than 1, and the predictions of the decision tree are multiplied by the learning rate before being added to the ensemble.\n",
    "\n",
    "5.Repeat: Steps 2-4 are repeated iteratively, with each new decision tree trained to predict the residuals of the ensemble formed by the previous trees. The predictions of each new tree are added to the predictions of the ensemble with appropriate weights, determined by the learning rate.\n",
    "\n",
    "6.Stopping Criterion: The iterative process continues until a stopping criterion is met, such as reaching a maximum number of trees or achieving satisfactory performance on a validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
